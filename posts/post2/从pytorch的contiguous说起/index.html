<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>从pytorch的contiguous说起 - ykddd&#39;s blog</title><meta name="Description" content=""><meta property="og:title" content="从pytorch的contiguous说起" />
<meta property="og:description" content="前言 最近组里来的实习生没有内网权限，对于Pytorch的入门，平时我自己也没有一些梳理，所以准备写一个教程demo，用于快速的上手一些Pyt" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/post2/%E4%BB%8Epytorch%E7%9A%84contiguous%E8%AF%B4%E8%B5%B7/" /><meta property="og:image" content="http://example.org/touxiang.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-13T21:57:54&#43;08:00" />
<meta property="article:modified_time" content="2022-01-13T21:57:54&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://example.org/touxiang.png"/>

<meta name="twitter:title" content="从pytorch的contiguous说起"/>
<meta name="twitter:description" content="前言 最近组里来的实习生没有内网权限，对于Pytorch的入门，平时我自己也没有一些梳理，所以准备写一个教程demo，用于快速的上手一些Pyt"/>
<meta name="application-name" content="ykddd&#39;s blog">
<meta name="apple-mobile-web-app-title" content="ykddd&#39;s blog"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://example.org/posts/post2/%E4%BB%8Epytorch%E7%9A%84contiguous%E8%AF%B4%E8%B5%B7/" /><link rel="prev" href="http://example.org/posts/post1/2022/" /><link rel="next" href="http://example.org/posts/post3/%E8%93%84%E6%B0%B4%E6%B1%A0%E6%8A%BD%E6%A0%B7%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "从pytorch的contiguous说起",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/example.org\/posts\/post2\/%E4%BB%8Epytorch%E7%9A%84contiguous%E8%AF%B4%E8%B5%B7\/"
        },"genre": "posts","keywords": "Pytorch, contiguous","wordcount":  1181 ,
        "url": "http:\/\/example.org\/posts\/post2\/%E4%BB%8Epytorch%E7%9A%84contiguous%E8%AF%B4%E8%B5%B7\/","datePublished": "2022-01-13T21:57:54+08:00","dateModified": "2022-01-13T21:57:54+08:00","publisher": {
            "@type": "Organization",
            "name": "ykddd"},"author": {
                "@type": "Person",
                "name": "ykddd"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="ykddd&#39;s blog"><span class="header-title-pre"><i class='fas fa-fish'></i></span><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"><i class='fas fa-book-open'></i> 文章 </a><a class="menu-item" href="/about/"><i class='fas fa-address-book'></i> 关于 </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="ykddd&#39;s blog"><span class="header-title-pre"><i class='fas fa-fish'></i></span><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title=""><i class='fas fa-book-open'></i>文章</a><a class="menu-item" href="/about/" title=""><i class='fas fa-address-book'></i>关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">从pytorch的contiguous说起</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>ykddd</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2022-01-13">2022-01-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 1181 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 3 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#前言">前言</a></li>
    <li><a href="#pytorch的contiguous">Pytorch的contiguous</a></li>
    <li><a href="#pytorch中的view类操作">Pytorch中的view类操作</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="前言">前言</h2>
<p>最近组里来的实习生没有内网权限，对于Pytorch的入门，平时我自己也没有一些梳理，所以准备写一个教程demo，用于快速的上手一些Pytorch适配需要关注的内容。同时也带着探索一些Pytorch的新特性。为了方便后文会混用Pytorch与pt。</p>
<h2 id="pytorch的contiguous">Pytorch的contiguous</h2>
<p>这篇文章总结的比较准确</p>
<blockquote>
<p>is_contiguous直观的解释是Tensor底层一维数组元素的存储顺序与Tensor按行优先一维展开的元素顺序是否一致。</p>
</blockquote>
<a href="https://zhuanlan.zhihu.com/p/64551412" target="_blank" rel="noopener noreffer">https://zhuanlan.zhihu.com/p/64551412</a>
<p>看完这篇文章，你应该清楚的几个事情是：<br>
1、pytorch连续的tensor的内存排布；<br>
2、连续在pytorch中的定义；<br>
3、为什么要有连续的定义。<br>
stride这个定义也就随之而来，它被Pytorch用来判断contiguous，代码如下：<br>
c10/core/TensorImpl.h</p>
<div class="highlight"><pre class="chroma"><code class="language-C++" data-lang="C++"><span class="n">TENSORIMPL_MAYBE_VIRTUAL</span> <span class="kt">bool</span> <span class="nf">is_contiguous</span><span class="p">(</span>
    <span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span> <span class="n">memory_format</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">Contiguous</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">C10_UNLIKELY</span><span class="p">(</span>
            <span class="n">has_contiguity_</span> <span class="o">!=</span>
            <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">HasContiguityPolicy</span><span class="o">::</span><span class="n">Default</span><span class="p">)))</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">is_contiguous_nondefault_policy_impl</span><span class="p">(</span><span class="n">memory_format</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">TORCH_INTERNAL_ASSERT_DEBUG_ONLY</span><span class="p">(</span><span class="n">compute_contiguous</span><span class="p">()</span> <span class="o">==</span> <span class="n">is_contiguous_</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">memory_format</span> <span class="o">==</span> <span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">ChannelsLast</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">is_channels_last_contiguous_</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">memory_format</span> <span class="o">==</span> <span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">ChannelsLast3d</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">is_channels_last_3d_contiguous_</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">is_contiguous_</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div><p>注意这个文件TensorImpl.h，Pytorch的tensor的定义核心就在于这个文件。对于Pytorch的扩展，官方也是要求你去继承这个文件，所以这就是为啥会有个
TENSORIMPL_MAYBE_VIRTUAL宏，方便你重写这个函数。这个文件需要耗费很多篇幅去写，所以暂时我们先搁置。<br>
先看入参，at::MemoryFormat这个也是需要花篇幅去讲述的。感兴趣可以先看这篇文章<a href="https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html" target="_blank" rel="noopener noreffer">https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html</a><br>
at::MemoryFormat::Contiguous是pt默认的内存存储格式，你也可以将它认为是nchw格式。而at::MemoryFormat::ChannelsLast和at::MemoryFormat::ChannelsLast3d则是nhwc和ndhwc格式。<br>
可以看到不同的内存存储格式有着不同的判断连续的标准，判断连续的逻辑被封装到了compute_contiguous()函数中。</p>
<div class="highlight"><pre class="chroma"><code class="language-C++" data-lang="C++"><span class="kt">bool</span> <span class="n">TensorImpl</span><span class="o">::</span><span class="n">compute_contiguous</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
  <span class="kt">bool</span> <span class="n">is_contiguous</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">is_empty</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">is_contiguous</span><span class="p">;</span>
  <span class="kt">int64_t</span> <span class="n">z</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">d</span> <span class="o">=</span> <span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">d</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="k">auto</span> <span class="n">size_d</span> <span class="o">=</span> <span class="n">sizes_and_strides_</span><span class="p">.</span><span class="n">size_at_unchecked</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">size_d</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">sizes_and_strides_</span><span class="p">.</span><span class="n">stride_at_unchecked</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">z</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">z</span> <span class="o">*=</span> <span class="n">size_d</span><span class="p">;</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">is_contiguous</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
        <span class="k">break</span><span class="p">;</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">is_contiguous</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div><p>可以看下这个判断的逻辑是否和上文链接中描述的相同。</p>
<h2 id="pytorch中的view类操作">Pytorch中的view类操作</h2>
<p><a href="https://pytorch.org/docs/stable/tensor_view.html" target="_blank" rel="noopener noreffer">https://pytorch.org/docs/stable/tensor_view.html</a><br>
以上是Pytorch中所定义的所有的View类操作。其中有这么一句话：</p>
<blockquote>
<p>PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.</p>
</blockquote>
<p>Pytorch通过stride与共享storage来实现了多个视图共享一块内存，每个视图又可以表示一种内存的获取方式，这里的storage指的是TensorImpl类中的成员变量，当然，这是后面我们需要介绍的。从contiguous谈起，会让我们有无数的疑问，这些疑问让我们后续慢慢探索。contiguous make us curious。</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2022-01-13</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/pytorch/">Pytorch</a>,&nbsp;<a href="/tags/contiguous/">contiguous</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/post1/2022/" class="prev" rel="prev" title="2022"><i class="fas fa-angle-left fa-fw"></i>2022</a>
            <a href="/posts/post3/%E8%93%84%E6%B0%B4%E6%B1%A0%E6%8A%BD%E6%A0%B7%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/" class="next" rel="next" title="蓄水池抽样算法简介">蓄水池抽样算法简介<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><i class='fas fa-calculator'></i>------------做一个有底线的人------------<i class='fas fa-calculator'></i></div><div class="footer-line"></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/typeit/typeit.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"data":{"id-1":"MoYu Master","id-2":"MoYu Master"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":null,"cursorSpeed":null,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":null,"speed":null}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
